{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-CIb-pub_0F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Iterable, List\n",
        "import math\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import Tensor\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #set the device as GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jok77llycOGB"
      },
      "outputs": [],
      "source": [
        "EMB_SIZE = 400\n",
        "NHEAD = 2\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 16\n",
        "NUM_ENCODER_LAYERS = 4\n",
        "NUM_DECODER_LAYERS = 4\n",
        "lr = 0.0001\n",
        "betas = (0.9, 0.98)\n",
        "eps = 1e-9\n",
        "NUM_EPOCHS = 36 #the result given will come if NUM_EPOCHS is set to 36.\n",
        "split_size = 0.1\n",
        "\n",
        "SRC_LANGUAGE = 'hi'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# vocab_size = {}\n",
        "# vocab_size[SRC_LANGUAGE] = 7000\n",
        "# vocab_size[TGT_LANGUAGE] = 7000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM_1MYZAaEHF"
      },
      "outputs": [],
      "source": [
        "#uncomment the following to mount while using colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3i5QvC8VAqS"
      },
      "outputs": [],
      "source": [
        "!mkdir -p \"/content/drive/My Drive/My Folder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_wJIlrqcQgO"
      },
      "outputs": [],
      "source": [
        "Lang = 'All'\n",
        "answer_filename = '/content/drive/My Drive/My Folder/answer_'+Lang +'.txt'\n",
        "model_filename = '/content/drive/My Drive/My Folder/model_'+Lang +'.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHFZbqbwc-hf"
      },
      "outputs": [],
      "source": [
        "#loading data from the desired directory\n",
        "DATA_PATH = '/content/drive/MyDrive/Machine_Translation_Data/English_'+Lang+'.csv'\n",
        "# TEST_PATH = '/kaggle/input/cs779-mt/eng_Hindi_data_dev_X.csv'\n",
        "FINAL_TEST_DATA = '/content/drive/MyDrive/Machine_Translation_Data/English_'+Lang+'_val.csv'\n",
        "data = pd.read_csv(DATA_PATH, header = None)\n",
        "data.columns = ['hindi', 'english']\n",
        "\n",
        "# test = pd.read_csv(TEST_PATH, header = None)\n",
        "# test.columns = ['sentence']\n",
        "final_test_data = pd.read_csv(FINAL_TEST_DATA, header = None)\n",
        "final_test_data.columns = ['sentence']\n",
        "data.head()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQu11ChUr28V"
      },
      "outputs": [],
      "source": [
        "def swap_columns(df, col1, col2):\n",
        "    col_list = list(df.columns)\n",
        "    x, y = col_list.index(col1), col_list.index(col2)\n",
        "    col_list[y], col_list[x] = col_list[x], col_list[y]\n",
        "    df = df[col_list]\n",
        "    return df\n",
        "\n",
        "data = swap_columns(data, 'hindi', 'english')\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IAvDq2JBVpH"
      },
      "outputs": [],
      "source": [
        "data['hindi']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BopFDyDR_ygL"
      },
      "outputs": [],
      "source": [
        "data['english'] = data['english'].apply(str)\n",
        "data['hindi'] = data['hindi'].apply(str)\n",
        "final_test_data['sentence'] = final_test_data['sentence'].apply(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF7E3Wan9TJg"
      },
      "outputs": [],
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "# The path to the local git repo for Indic NLP library\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "\n",
        "# The path to the local git repo for Indic NLP Resources\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue7w-tQ63p9y"
      },
      "outputs": [],
      "source": [
        "#train test split using sklearn\n",
        "train, eval = train_test_split(data, test_size = split_size, random_state = 42)\n",
        "train = train.reset_index(drop = True)\n",
        "eval = eval.reset_index(drop = True)\n",
        "print(train.shape)\n",
        "print(eval.shape)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcUSb4mI3TRV"
      },
      "outputs": [],
      "source": [
        "#defining the iterable class for creating the iterable dataset\n",
        "#it takes two series as input namely hindi and english sentence series and\n",
        "#generates a tuple of source and target sentence as follows\n",
        "class MyIterableDataset(IterableDataset):\n",
        "    def __init__(self, english_sentences, hindi_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.hindi_sentences = hindi_sentences\n",
        "        self.index = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.index >= len(self.english_sentences):\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            english_sentence = self.english_sentences[self.index]\n",
        "            hindi_sentence = self.hindi_sentences[self.index]\n",
        "            self.index += 1\n",
        "            return hindi_sentence, english_sentence\n",
        "\n",
        "# Example usage\n",
        "train_iter = MyIterableDataset(train['english'], train['hindi'])\n",
        "eval_iter = MyIterableDataset(eval['english'], eval['hindi'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ciihdBj98qQ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "def engTokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize an English text and return a list of tokens\n",
        "    \"\"\"\n",
        "    return [str(token.text) for token in eng.tokenizer(str(text))]\n",
        "\n",
        "def hiTokenize(text):\n",
        "    \"\"\"\n",
        "    Tokenize a German text and return a list of tokens\n",
        "    \"\"\"\n",
        "    return [str(t) for t in indic_tokenize.trivial_tokenize(str(text))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ0QbyUg_Mfm"
      },
      "outputs": [],
      "source": [
        "# def getTokens(data_iter, place):\n",
        "#     \"\"\"\n",
        "#     Function to yield tokens from an iterator. Since, our iterator contains\n",
        "#     tuple of sentences (source and target), `place` parameters defines for which\n",
        "#     index to return the tokens for. `place=0` for source and `place=1` for target\n",
        "#     \"\"\"\n",
        "#     for english, german in data_iter:\n",
        "#         if place == 0:\n",
        "#             yield engTokenize(english)\n",
        "#         else:\n",
        "#             yield hiTokenize(german)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFJQouRvEwmb"
      },
      "outputs": [],
      "source": [
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "token_transform[SRC_LANGUAGE] = hiTokenize\n",
        "token_transform[TGT_LANGUAGE] = engTokenize\n",
        "\n",
        "# function to generate the tokens for each language\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    #create the iterator object of the dataset given\n",
        "    train_iter = MyIterableDataset(train['english'], train['hindi'])\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=2,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True,\n",
        "                                                    # max_tokens = vocab_size[ln]\n",
        "                                                    )\n",
        "\n",
        "#setting the default index to unknown index which means that it will assume the token to be unknown if\n",
        "#it sees a word not in the dictionary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKmg9N6Qyel0"
      },
      "outputs": [],
      "source": [
        "len(vocab_transform['en'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ4vOSVZO-W0"
      },
      "outputs": [],
      "source": [
        "print(\"tokenized hindi sentence \", token_transform['en'](train['english'][0]))\n",
        "print(\"numericalized hindi sentence \", vocab_transform['en'](token_transform['hi'](train['english'][0])))\n",
        "#check for the correct tokenization and numericalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taLWE2OLaCJJ"
      },
      "outputs": [],
      "source": [
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 7000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# The Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SbdK3S-aCJP"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "#mask creation for preventing the knowledge of presence of elements in future time steps\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dR-tgFmjaCJS"
      },
      "outputs": [],
      "source": [
        "#hyper-paramerter setting\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "print(SRC_VOCAB_SIZE)\n",
        "print(TGT_VOCAB_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "#creating the model with the hyperparams specified as above\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "# initializes the weight matrices of the transformer model using Xavier uniform initialization\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE) #push the model to the device\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX) #cross entropy loss\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=lr, betas=betas, eps=eps) #adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2Ter7p3aCJV"
      },
      "outputs": [],
      "source": [
        "'''creating the collate function to be passed in the dataloader which will basically apply\n",
        "this function to every entry of the batch and make the data feedable to the model\n",
        "'''\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p-mjV1I3ZLm"
      },
      "outputs": [],
      "source": [
        "train_iter = MyIterableDataset(train['english'], train['hindi'])\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "count = 1\n",
        "for src, tgt in train_dataloader:\n",
        "  if count == 5:\n",
        "    break\n",
        "  print(src.shape)\n",
        "  count+=1\n",
        "\n",
        "#checking the shapes of different batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJxQj2U1IOWo"
      },
      "outputs": [],
      "source": [
        "ntrain = train.shape[0]\n",
        "neval = eval.shape[0]\n",
        "print(ntrain)\n",
        "print(neval)\n",
        "\n",
        "#total number of samples in the train and eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qutjpTXIkEf"
      },
      "outputs": [],
      "source": [
        "ntrainbatches = int(np.ceil(ntrain/BATCH_SIZE))\n",
        "nevalbatches = int(np.ceil(neval/BATCH_SIZE))\n",
        "print(\"number of train batches \", ntrainbatches)\n",
        "print(\"number of eval batches \", nevalbatches)\n",
        "\n",
        "#total number of batches in the train and eval dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdhtswLKaCJZ"
      },
      "outputs": [],
      "source": [
        "# Functions for training and evaluation on the whole dataset for one epoch\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = MyIterableDataset(train['english'], train['hindi'])\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    for src, tgt in train_dataloader:\n",
        "\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    print(\"train losses \", losses)\n",
        "    return losses / ntrainbatches\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "\n",
        "    eval_iter = MyIterableDataset(eval['english'], eval['hindi'])\n",
        "    val_dataloader = DataLoader(eval_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "    print(\"validation losses \", losses)\n",
        "    return losses / nevalbatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGyQpL0KTirY"
      },
      "outputs": [],
      "source": [
        "!export CUDA_LAUNCH_BLOCKING=1 #might give error some time, just comment out if it does so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2o2Vav-4_mw"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rTSyNGN47lW"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VX3UFpM4aCJd"
      },
      "outputs": [],
      "source": [
        "#training loop\n",
        "prev_val_loss = 0\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    if epoch == 1:\n",
        "        prev_val_loss = val_loss\n",
        "    else:\n",
        "        e = prev_val_loss - val_loss\n",
        "        if e < 0.001:\n",
        "            break\n",
        "        prev_val_loss = val_loss\n",
        "\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz5_xoS51zKC"
      },
      "outputs": [],
      "source": [
        "torch.save(transformer.state_dict(),model_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SutpsX_3TirY"
      },
      "outputs": [],
      "source": [
        "'''functions for decoding the final output tensor into the english sentence. We have used\n",
        "two types of decoding techniques namely beam search decode and greedy decode. Although\n",
        "we have used only the greedy decode scheme for our purpose for the reason that it takes\n",
        "less '''\n",
        "\n",
        "import heapq\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def beam_search_decode(model, src, src_mask, max_len, start_symbol, beam_size=3):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\n",
        "    # Initialize the beam with a single hypothesis\n",
        "    beams = [(0.0, ys)]\n",
        "\n",
        "    # Repeat beam expansion until max_len or EOS is reached\n",
        "    for i in range(max_len-1):\n",
        "        new_beams = []\n",
        "        for score, ys in beams:\n",
        "            # Check if the last token in the sequence is EOS\n",
        "            if ys[-1] == EOS_IDX:\n",
        "                new_beams.append((score, ys))\n",
        "                continue\n",
        "\n",
        "            memory = memory.to(DEVICE)\n",
        "            tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                        .type(torch.bool)).to(DEVICE)\n",
        "            tgt_mask = tgt_mask.unsqueeze(0)  # Add a new dimension at index 0\n",
        "            tgt_mask = tgt_mask.repeat(2, tgt_mask.shape[1], tgt_mask.shape[1])\n",
        "            print(ys.unsqueeze(0).shape, memory.squeeze(1).shape, tgt_mask.shape)\n",
        "            out = model.decode(ys, memory, tgt_mask)\n",
        "            out = out.squeeze(0)\n",
        "            prob = model.generator(out[-1])\n",
        "            top_probs, top_idxs = torch.topk(prob, beam_size)\n",
        "\n",
        "            # Expand the beam with each possible next token\n",
        "            for j in range(beam_size):\n",
        "                next_word = top_idxs[j].item()\n",
        "                score_j = score + top_probs[j].item()\n",
        "                p = torch.tensor([next_word]).type_as(src.data)\n",
        "                p = p.unsqueeze(0)\n",
        "                print(ys.shape, p.shape)\n",
        "                ys_j = torch.cat([ys, p], dim=0)\n",
        "                new_beams.append((score_j, ys_j))\n",
        "\n",
        "        # Keep only the top beam_size hypotheses\n",
        "        beams = heapq.nlargest(beam_size, new_beams, key=lambda x: x[0])\n",
        "\n",
        "    # Return the hypothesis with the highest score\n",
        "    return beams[0][1]\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0oLOnLtaCJe"
      },
      "outputs": [],
      "source": [
        "#example of translation\n",
        "# print(translate(transformer, \"ईमान लाओ और उसके रसूल के साथ होकर जिहाद करो\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmytiJK8ZUbW"
      },
      "outputs": [],
      "source": [
        "#loading the model using pickle deserialization\n",
        "transformer.load_state_dict(torch.load(model_filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdo5TXHmZlRd"
      },
      "outputs": [],
      "source": [
        "#checking whether the loaded model is same as the transformer\n",
        "print(final_test_data['sentence'][25], translate(transformer, final_test_data['sentence'][25]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNkUYQnsTirZ"
      },
      "outputs": [],
      "source": [
        "'''calculation of bleu score , uncomment the following code snippet and change the\n",
        "hypothesis and reference sentence data in the corresponding fields to calculate the\n",
        "corpus bleu score'''\n",
        "\n",
        "# from torchtext.data.metrics import bleu_score\n",
        "# actual = [token_transform['hi'](sentence) for sentence in eval['english'][:2000]]\n",
        "# prediction = [token_transform['en'](translate(transformer, sentence)) for sentence in eval['hindi'][:2000]]\n",
        "# Compute individual n-gram scores and their geometric mean\n",
        "# BLEU-4\n",
        "\n",
        "# print(f\"BLEU score: {score: f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75jCKMD8TirZ"
      },
      "outputs": [],
      "source": [
        "#saving the predicted answers of the final_test_data\n",
        "count = 0\n",
        "with open(answer_filename, 'w', encoding = 'utf-8') as f:\n",
        "  for sentence in final_test_data['sentence']:\n",
        "    translated = translate(transformer, sentence)\n",
        "    # print(type(translated))\n",
        "    count+=1\n",
        "    f.write(translated + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBjPsM6eeCAu"
      },
      "outputs": [],
      "source": [
        "print(len(final_test_data['sentence']))\n",
        "print(count)\n",
        "\n",
        "#checking the count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7UjUl7_pSdt"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "with open(answer_filename, 'r', encoding = 'utf-8') as f:\n",
        "  for line in f:\n",
        "    c+=1\n",
        "print(c)\n",
        "\n",
        "#rechecking the count by loading the answer.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}